{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e7cb49",
   "metadata": {},
   "source": [
    "# Análise e Exploração dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ac6f40e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6388/2308266231.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "# Importações\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766217e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a959bd04",
   "metadata": {},
   "source": [
    "### Importando os datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92131b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "falsas = pd.read_csv('./datasets/falsas.csv')\n",
    "falsas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a659ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdadeiras = pd.read_csv('./datasets/verdadeiras.csv')\n",
    "verdadeiras.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2872eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('./datasets/merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df99e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\033[1m' + 'Dataset de Notícias Falsas' + '\\033[0m')\n",
    "\n",
    "print('\\nExistem {} observações e {} features. \\n'.format(falsas.shape[0], falsas.shape[1]))\n",
    "\n",
    "categorias = falsas.groupby('Categoria').count()\n",
    "\n",
    "print(f'Quantidade de registros por categoria:\\n {categorias}')\n",
    "\n",
    "print('-------------------------------------------------')\n",
    "\n",
    "print('\\033[1m' + 'Dataset de Notícias Verdadeiras' + '\\033[0m')\n",
    "\n",
    "print('\\nExistem {} observações e {} features. \\n'.format(verdadeiras.shape[0], verdadeiras.shape[1]))\n",
    "\n",
    "categorias = verdadeiras.groupby('Categoria').count()\n",
    "\n",
    "print(f'Quantidade de registros por categoria:\\n {categorias}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#?WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a56661",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"portuguese\")\n",
    "outras_stopwords = ['sobre', 'pode', 'ainda', 'após', 'ano', 'diz', 'disse', 'vai', 'anos', 'país','whatsapp', 'outro', 'desde', 'durante']\n",
    "stopwords.extend(outras_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf13ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = ''.join(resumo for resumo in falsas.Resumo)\n",
    "print ('Existem {} palavras na combinação de todos os resumos.'.format(len(textos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb320fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(textos)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70b6b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = ''.join(titulo for titulo in falsas.Titulo)\n",
    "print ('Existem {} palavras na combinação de todos os resumos.'.format(len(textos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(textos)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1616bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = ''.join(resumo for resumo in verdadeiras.Resumo)\n",
    "print ('Existem {} palavras na combinação de todos os resumos.'.format(len(textos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(textos)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07578df",
   "metadata": {},
   "outputs": [],
   "source": [
    "falsas['Titulo'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858df21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdadeiras['Titulo'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccf3fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "falsas['Resumo'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcce11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdadeiras['Resumo'].str.len().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca3b91",
   "metadata": {},
   "source": [
    "Comprimento médio das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "falsas['Titulo'].str.split().\\\n",
    "   apply(lambda x : [len(i) for i in x]). \\\n",
    "   map(lambda x: np.mean(x)).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdadeiras['Titulo'].str.split().\\\n",
    "   apply(lambda x : [len(i) for i in x]). \\\n",
    "   map(lambda x: np.mean(x)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19420ad",
   "metadata": {},
   "source": [
    "Ngramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import seaborn as sns\n",
    "\n",
    "def get_top_ngram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) \n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f0f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_bigrams=get_top_ngram(falsas['Titulo'],2)[:10] \n",
    "x,y=map(list,zip(*top_n_bigrams)) \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744263d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_bigrams=get_top_ngram(verdadeiras['Titulo'],2)[:10] \n",
    "x,y=map(list,zip(*top_n_bigrams)) \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8222bcf1",
   "metadata": {},
   "source": [
    "Exploração de modelagem de tópicos com pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1687eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b9af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import gensim\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "def preprocessa_noticia(df):\n",
    "    corpus = []\n",
    "    stem = PorterStemmer()\n",
    "    lem = WordNetLemmatizer()\n",
    "    for news in df['Titulo']:\n",
    "        words=[w for w in word_tokenize(news) if (w not in stop)]\n",
    "        \n",
    "        words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "        \n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "corpus_falsas = preprocessa_noticia(falsas)\n",
    "verdadeiras = preprocessa_noticia(verdadeiras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2515b106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelo bag of words usando o gensim\n",
    "\n",
    "dic=gensim.corpora.Dictionary(corpus)\n",
    "bow_corpus = [dic.doc2bow(doc) for doc in corpus]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
